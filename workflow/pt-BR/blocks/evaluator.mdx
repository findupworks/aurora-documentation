---
title: Avaliador
---

O bloco Avaliador utiliza IA para pontuar e avaliar a qualidade do conteúdo através de métricas de avaliação personalizáveis que você define. Perfeito para controle de qualidade, testes A/B e para garantir que seus resultados de IA atendam a padrões específicos.

<div className="flex justify-center my-6">![API](/images/evaluator.png)</div>

## Visão Geral

O bloco Avaliador permite:

<Steps>
  <Step>
    <strong>Pontuar a qualidade do conteúdo</strong>: Usa IA para avaliar
    conteúdo de acordo com métricas personalizadas com pontuações numéricas
  </Step>
  <Step>
    <strong>Definir métricas personalizadas</strong>: Cria critérios de
    avaliação específicos adaptados ao seu caso de uso
  </Step>
  <Step>
    <strong>Automatizar o controle de qualidade</strong>: Constrói fluxos de
    trabalho que avaliam e filtram conteúdo automaticamente
  </Step>
  <Step>
    <strong>Acompanhar o desempenho</strong>: Monitora melhorias e consistência
    ao longo do tempo com pontuações objetivas
  </Step>
</Steps>

## Como funciona

O bloco Avaliador processa conteúdo através de avaliação impulsionada por IA:

<Steps>
  <Step>
    **Recebe conteúdo** - Obtém o conteúdo de entrada de blocos anteriores no
    seu fluxo de trabalho
  </Step>
  <Step>
    **Aplica métricas** - Avalia o conteúdo de acordo com suas métricas
    personalizadas definidas
  </Step>
  <Step>
    **Gera pontuações** - O modelo de IA atribui pontuações numéricas para cada
    métrica
  </Step>
  <Step>
    **Fornece resumo** - Retorna uma avaliação detalhada com pontuações e
    explicações
  </Step>
</Steps>
## Opções de configuração

### Métricas de avaliação

Defina métricas personalizadas para avaliar o conteúdo. Cada métrica inclui:

- **Nome**: Um identificador curto para a métrica
- **Descrição**: Uma explicação detalhada do que a métrica mede
- **Intervalo**: O intervalo numérico para a pontuação (ex.: 1-5, 0-10)

Exemplos de métricas:

```
Accuracy (1-5): How factually accurate is the content?
Clarity (1-5): How clear and understandable is the content?
Relevance (1-5): How relevant is the content to the original query?
```

### Conteúdo

O conteúdo a avaliar. Isso pode ser:

- Fornecido diretamente na configuração do bloco
- Conectado a partir da saída de outro bloco (tipicamente um bloco de Agente)
- Gerado dinamicamente durante a execução do fluxo de trabalho

### Seleção de modelo

Escolha um modelo de IA para realizar a avaliação:

- **OpenAI**: GPT-4o, o1, o3, o4-mini, gpt-4.1
- **Anthropic**: Claude 3.7 Sonnet
- **Google**: Gemini 2.5 Pro, Gemini 2.0 Flash
- **Outros provedores**: Groq, Cerebras, xAI, DeepSeek
- **Modelos locais**: Qualquer modelo executando no Ollama

<div className="w-full max-w-2xl mx-auto overflow-hidden rounded-lg">
  <Video src="models.mp4" width={500} height={350} />
</div>

**Recomendação**: Use modelos com fortes capacidades de raciocínio como GPT-4o ou Claude 3.7 Sonnet para avaliações mais precisas.

### Chave API

Sua chave API para o provedor de LLM selecionado. Esta é armazenada de forma segura e utilizada para autenticação.

## Como funciona

1. O bloco Avaliador obtém o conteúdo fornecido e suas métricas personalizadas
2. Gera um prompt especializado que instrui o LLM a avaliar o conteúdo
3. O prompt inclui diretrizes claras sobre como pontuar cada métrica
4. O LLM avalia o conteúdo e retorna pontuações numéricas para cada métrica
5. O bloco Avaliador formata essas pontuações como saída estruturada para uso no seu fluxo de trabalho

## Exemplos de casos de uso

### Avaliação de qualidade de conteúdo

<div className="mb-4 rounded-md border p-4">
  <h4 className="font-medium">
    Cenário: Avaliar a qualidade de um artigo de blog antes de sua publicação
  </h4>
  <ol className="list-decimal pl-5 text-sm">
    <li>O bloco de Agente gera o conteúdo do artigo</li>
    <li>O Avaliador avalia a precisão, legibilidade e engajamento</li>
    <li>
      O bloco de Condição verifica se as pontuações atendem aos limites mínimos
    </li>
    <li>
      Pontuações altas → Publicar, Pontuações baixas → Revisar e tentar
      novamente
    </li>
  </ol>
</div>

### Testes A/B de conteúdo

<div className="mb-4 rounded-md border p-4">
  <h4 className="font-medium">
    Cenário: Comparar múltiplas respostas geradas por IA
  </h4>
  <ol className="list-decimal pl-5 text-sm">
    <li>O bloco paralelo gera múltiplas variações de resposta</li>
    <li>O avaliador pontua cada variação de acordo com clareza e relevância</li>
    <li>O bloco de função seleciona a resposta com maior pontuação</li>
    <li>O bloco de resposta retorna o melhor resultado</li>
  </ol>
</div>

### Controle de qualidade de atendimento ao cliente

<div className="mb-4 rounded-md border p-4">
  <h4 className="font-medium">
    Cenário: Garantir que as respostas de suporte atendam aos padrões de
    qualidade
  </h4>
  <ol className="list-decimal pl-5 text-sm">
    <li>O agente de suporte gera uma resposta à consulta do cliente</li>
    <li>O avaliador pontua a utilidade, empatia e precisão</li>
    <li>
      As pontuações são registradas para treinamento e monitoramento de
      desempenho
    </li>
    <li>Pontuações baixas ativam um processo de revisão humana</li>
  </ol>
</div>

## Entradas e saídas

<Tabs items={["Configuração", "Variáveis", "Resultados"]}>
  <Tab title="Configuração">
    <ul>
      <li>
        <strong>Conteúdo</strong>: O texto ou dados estruturados a avaliar
      </li>
      <li>
        <strong>Métricas de avaliação</strong>: Critérios personalizados com
        intervalos de pontuação
      </li>
      <li>
        <strong>Modelo</strong>: Modelo de IA para análise de avaliação
      </li>
      <li>
        <strong>Chave API</strong>: Autenticação para o provedor de LLM
        selecionado
      </li>
    </ul>
  </Tab>
  <Tab title="Variáveis">
    <ul>
      <li>
        <strong>evaluator.content</strong>: Resumo da avaliação
      </li>
      <li>
        <strong>evaluator.model</strong>: Modelo utilizado para a avaliação
      </li>
      <li>
        <strong>evaluator.tokens</strong>: Estatísticas de uso de tokens
      </li>
      <li>
        <strong>evaluator.cost</strong>: Resumo de custos para a chamada de
        avaliação
      </li>
    </ul>
  </Tab>
  <Tab title="Resultados">
    <ul>
      <li>
        <strong>Pontuações de métricas</strong>: Pontuações numéricas para cada
        métrica definida
      </li>
      <li>
        <strong>Resumo de avaliação</strong>: Avaliação detalhada com
        explicações
      </li>
      <li>
        <strong>Acesso</strong>: Disponível em blocos após o avaliador
      </li>
    </ul>
  </Tab>
</Tabs>

## Melhores práticas

- **Usar descrições específicas de métricas**: Defina claramente o que cada métrica mede para obter avaliações mais precisas
- **Escolher intervalos apropriados**: Selecione intervalos de pontuação que forneçam detalhes suficientes sem serem excessivamente complexos
- **Conectar com blocos de agente**: Use blocos avaliadores para avaliar as saídas de blocos de agente e criar loops de feedback
- **Usar métricas consistentes**: Para análises comparativas, mantenha métricas consistentes em avaliações similares
- **Combinar múltiplas métricas**: Use várias métricas para obter uma avaliação abrangente
